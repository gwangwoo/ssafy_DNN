### ssafy_DNN
Deep learning study

# 1강~2강

### 퍼셉트론(인공뉴런) : 1957년에 고안된 알고리즘

### 퍼셉트론은 딥러닝(신경망)의 기원이 되는 알고리즘

### 퍼셉트론은 다수의 신호를 입력받아 하나의 신호를 출력한다.

### 여기서 신호는 전류나 물의 흐름을 생각해 볼 수 있다.

### 신호의 흐름을 표현할 때 두가지값을 갖는데 0,1의 값을 갖는다.
### 0은 안흐른다. 1은 신호가 흐른다.

[ 퍼셉트론 식 ]

w1 * x1 + w2 * x2 ≤  Ø
w1 * x1 + w2 * x2  > Ø

x1,x2는 압력신호, y는 출력신호, w1,w2는 가중치를 의미한다.
(w는 weight를 의미) Ø는 임계값

w1 * x1 + w2 * x2 값이 임계값 이하 일때는 0
아닐 때는 1

가중치가 클수록 해당신호가 강해진다

[ 논리 회로 ]

 * AND 게이트 진리표
 
|x1|x2|y|
|---|---|---|
|0|0|0|
|0|1|0|
|1|0|0|
|1|1|1|

입력이 모두 1일때만 1을 출력한다.

퍼셉트론으로 AND게이트를 표현하고자 할 때는 w1, w2, Ø 값을 어떤 값으로 설정할 것인지 생각해 봐야 한다.

AND게이트를 만족하는 w1,w2,Ø의 조합은 무수히 많다.

 * NAND 게이트 진리표
 
|x1|x2|y|
|---|---|---|
|0|0|1|
|0|1|1|
|1|0|1|
|1|1|0|

 * OR 게이트 진리표
 
|x1|x2|y|
|---|---|---|
|0|0|0|
|0|1|1|
|1|0|1|
|1|1|1|

퍼셉트론 표현할 때는 가중치와 임계값을 설정하여 표현할 수 있다.

이 매개변수의 값(w, Ø)을 적절히 조절하면 AND, NAND, OR 게이트를 표현 할 수 있다.



# 3강

[ 가중치와 편향을 도입한 퍼셉트론 식 ]

편향 : bias

 * 위의 퍼셉트론 식에서 Ø를 -b로 치환하면

w1 * x1 + w2 * x2 + b  으로 바꿔 줄 수 있다.

퍼셉트론은 입력신호에 가중치를 곱한 값과 편향을 합하여, 그 값이 0을 넘으면 1을 출력하고, 그렇지 않으면 0을 출력한다.


# 4강

 * XOR 게이트 진리표
|x1|x2|y|
|------|---|---|
|0|0|0|
|0|1|1|
|1|0|1|
|1|1|0|
 
 XOR 게이트는 배타적논리합이라고 한다
 x1과 x2 중 어느 한쪽이 1일 때만 1을 출력하는 논리회로
 
 퍼셉트론으로는 XOR 게이트를 구현 할 수 없다.
 
 직선 하나로는 XOR 게이트의 출력을 구분할 수 없다.(구분 불가능)
 
 퍼셉트론(단층 퍼셉트론)은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있다.
 
 
 ![100](/img/100.jpg)
 
 
 
 하지만 시간이 지나고 비선형 그래프로 XOR문제를 풀 수 있게 된다.
 
 다층 퍼셉트론을 사용하면 나눌 수 있게 된다.
 
 ![101](/img/101.png)
 
 
 
 선형  : 직선의 영역을 선형 영역
 비선형 : 곡선의 영역을 비선형 영역
 
 
# 5강
 
 ### 다층 퍼셉트론(Multi Layer Perceptron)
 
 * 단층 퍼셉트론으로는 XOR게이트를 표현할 수 없다.
   즉, 단층 퍼셉트론으로는 비선형 영역을 분리할 수 없다.
 
 기존 게이트 (AND, OR, NAND) 조합하여 층을 쌓으면 XOR 게이트를 구현 할 수 있게 된다.
 
|x1|x2|s1|s2|y|
|---|---|---|---|---|
|0|0|1|0|0|
|0|1|1|1|1|
|1|0|1|1|1|
|1|1|0|1|0|


 
 
# 6강

퍼셉트론과 딥러닝의 차이는 -> 가중치를 스스로 학습을 통해 변경한다


### 신경망

입력층과 은닉층 출력층으로 이루어져 있다.

은닉층(사람 눈에 보이지 않는 층)은 중간층을 말한다.

입력층(0층), 은닉층(1층), 출력층(2층)은 앞시간에 배웠던 퍼셉트론의 신호전달 방식과 유사하다.(동일하다)
 
활성화 함수를 이용한 퍼셉트론 식

y = h(b + w1*x1 + w2*x2)

y = h(x) -> 활성화 함수.(0,1)의 값을 갖게 된다.

입력 신호의 총합을 출력신호로 변환하는 함수를 "활성화 함수" 라고 한다.

a = b + w1*x1 + w2*x2 (입력신호 총합)

y = h(a)

위의 식은 가중치가 있는 입력신호와 편향을 계산하고 이를 a 라고 하면 a를 함수 h()에 넣어 y를 출력하는 흐름을 보여준다.


[ 시그 모이드 함수 ]

신경망에서 자주 이용하는 활성화 함수 중의 하나이다.

h(x) = 1 / 1+e<sup>-x</sup>

![103](/img/103.png)


# 7강

[ 계단 함수 ]
임계값을 경계로 출력이 바뀌는 함수

numpy.astype(np.int) 타입을 바꿔주는 함수.

![102](/img/102.png)



# 8강

퍼셉트론에서는 활성화함수가 --> 계단함수 역할

딥러닝(신경망)에서는 활성화함수가 --> 시그모이드, 렐루 등등


### 계단함수와 시그모이드 함수의 공통점
 ** 비선형 함수: 직선하나로는 그릴 수 없는 함수
 
 따라서, 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.
 
 만약 선형함수인 경우에는, 은닉층이없는 것과 같다. a = b<sup>3</sup>
 



[ ReLU 함수 ]

 - Rectified Linear Unit 함수 : 입력이 0이 넘으면 그 입력을 그대로 출력하고 0 이하이면 0을 출력하는 함수.
 
     h(x) = <sup>x (x가 0보다 크면)</sup>
            <sub>0 (x가 0보다 작으면)</sub>
            
            

# 9강

[ 행렬의 곱 (행렬의 내적) ]

 - 2차원 배열의 내적(행렬의 곱)
 
    * 행렬의 내적은 형상에 주의 해야 한다.
    
    행렬의 내적은 대응하는 차원의 원수 소를 일치 시켜야 한다.
    
    (n * p) • (p * m) ==> 일치해야함
    
    
# 10강


[ 신경망의 내적(가중치만 적용) ]

X = np.array([1,2]) # 입력신호
W = np.array([[1,3,5],[2,4,6]]) # 1,3,5는 x1의 가중치, 2,4,6은 x2의 가중치

Y = np.dot(X,W)

![104](/img/104.png)


# 11강

활성화 함수를 사용하지 않은 경우.
![105](/img/105.png)


# 12강

활성화 함수를 사용한 경우.

1층에서 2층으로의 신호 전달

![106](/img/106.png)


2층에서 출력층으로의 신호 전달

![107](/img/107.png)


# 13강

마지막 층(출력층)에서는 '활성화함수'가 아닌 '항등함수'로 사용한다.



# 14강

기계 학습의 문제:

- 분류(classification)
- 회귀(regression)


** 신경망에서는 회귀에 사용하는 활성함수로 항등함수를 사용하며,
   분류에 사용하는 활성함수는 소프트맥스 함수를 사용한다.

![108](/img/108.png)

[ 소프트 맥스 함수 정의 ]

def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

[ 개선된 소프트 맥스 함수 ]

![109](/img/109.png)

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y


# 15강

[ 소프트맥스 함수의 특징 ]


import numpy as np

### softmax func
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

a = np.array([3.0,4.0,2.1,5.3,6.3])
y = softmax(a)
print(y)


소프트 맥수 함수에 의한 출력은 0에서 1.0사이의 실수 값이 출력된다.
이때의 출력밧의 총 합은 항상 1이 된다는 점이 소프트맥스 함수의 특징이다.
이 특징은 이함수의 중요한 특성이다.

즉, 이 출력값은 확률로 해석할 수 있다. 문제를 통계적으로 대응할 수 있다.

<b>각 원소의 대소 관계는 소프트맥스 함수의 출력값과 동일하다.</b>

신경망을 이용한 분류에서는 출력층의 소프트맥스 함수를 생략해도 무방하다.

소프트 맥스 함수를 적용하지 않더라도 결과는 동일 하기 때문이다.

현업에서는 실질적으로 지수 함수 계산에 드는 자원낭비를 줄일 수 있기 때문에 일반적으로 출력층에서의 소프트맥스 함수를 생략하여 사용한다.



# 16강

### Tensorflow : 구글이 오픈소스로 공개한 머신러닝 라이브러리

    - 딥러닝을 비롯한 여러 머신러닝에 사용되고 있는 라이브러리

    - 대규모 숫자 계산을 해주는 라이브러리
    
    - 자체는 C++로 만들어진 라이브러리
    
    - 실질적으로는 딥러닝 용도로 많이 사용
    
    - 일반적인 프로그래밍 방식과는 약간 다른 개념들 포함한다.
    
    - tensor, placeholder, 변수
    
    
[ 설치 과정 ]
pip3 install --upgrade tensorflow

[ tensorflow 사용하기 ]
import tensorflow as tf

[ tensorflow의 자료형 ]
 - tensor : tensorflow에서 다양한 수학식을 계산하기 위한 가장 기본적이고 중요한 자료형.
   tensor의 형태는 배열과 비슷
   
   Rank와 Shape이라는 개념을 가지고 있다.
   
   [Rank 값]
   0 : 스칼라
   1 : 벡터
   2 : 행렬
   3 이상 : n-Tensor 또는 n차원 텐서
   [ Shape ]
   각 차원의 요소 개수를 의미, 텐서의 구조를 설명
   
   [dtype]
   해당 텐서에 담긴 요소들의 자료형
   string, float, int 등이 올 수 있다.
   
   

